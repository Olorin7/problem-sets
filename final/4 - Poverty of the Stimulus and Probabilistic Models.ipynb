{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". As a reminder, there is **NO COLLABORATION** whatsoever on the final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part A (3 points)\n",
    "\n",
    "Chomsky's argument from the poverty of the stimulus is one of the most\n",
    "famous and controversial claims in cognitive science. Many researchers\n",
    "view formal results on learnability as supporting the argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">In one paragraph, briefly summarize the poverty of the stimulus\n",
    "argument, and outline the implications of the argument for language\n",
    "learning.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c7fb8d2fcf4e905fd28936598fa25f74",
     "grade": true,
     "grade_id": "sumPOS",
     "points": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part B (2 points)\n",
    "\n",
    "In analyses of language learnability, cases where one language is a subset of another are particularly challenging. Imagine that a learner is trying to decide between two languages. Language $L_1$ consists of all the grammatical sentences in English. Language $L_2$ consists of all of the grammatical sentences in English, plus \"Furiously sleep ideas green colorless.\" We can express this relationship between the languages graphically:\n",
    "<br />\n",
    "\n",
    "![](images/subsetProblem.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Assume that the learner only receives positive examples—sentences that are valid examples of $L_1$—and wants to choose between $L_1$ and $L_2$. Explain why this creates a problem for approaches to learning that view a language as a set and attempt to logically deduce that set from examples.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e02642e94bb077d76aade0a7df0c1cf1",
     "grade": true,
     "grade_id": "subsetProblem",
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part C (2 points)\n",
    "\n",
    "Now, assume that the learner views $L_1$ and $L_2$ as probability distributions over sentences. Under $L_1$, sentences are generated from the probability distribution observed in English. Under $L_2$, $c$ is the probability that the specific sentence \"Furiously sleep ideas green colorless\" is generated, and $1-c$ is the probability that a sentence is generated from the probability distribution observed in English. Thus, the probability of any particular sentence in English in $L_2$ is $(1-c)$ times the probability of that sentence in $L_1$ (in other words, if the sentence in English is $s$, then $P(s|L_2) = P(s|L_1)(1-c)$). The problem of learning a language can now be formulated as a problem of Bayesian inference. The hypotheses $h$ are languages, and the data $d$ are a set of sentences. The likelihood $P(d|h)$ is the probability of a set of sentences given a language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "In this case we are interested in the <i>posterior odds ratio</i>, or how many times more likely is $L_1$ than $L_2$. Recall posterior odds are given by the equation: \n",
    "\n",
    "$$\\frac{P(L_1~|~d)}{P(L_2~|~d)} = \\frac{P(d~|~L_1)P(L_1)}{P(d~|~L_2)P(L_2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\"> Complete the function `getPostOdds` so that it computes the posterior odds ratio of $L_1$ over $L_2$ after observing $n$ consecutive English sentences. Assume that the sentences are generated independently from the distributions associated with the two languages, and that the probability of generating 'Furiously sleep ideas green colorless' under $L_2$, as denoted by the argument $c$, is .001.  Assume that the two languages, $P(L_1)$ and $P(L_2)$, are equally likely <i>a priori</i>. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0ed83e98ef99093105126161a77785b",
     "grade": false,
     "grade_id": "getPostOdds",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def getPostOdds(n, c=.001):    \n",
    "    \"\"\"Compute the posterior odds ratio of L1 (all grammatical sentences \n",
    "    in English) / L2 (all grammatical sentences in English + 'Furiously \n",
    "    sleep ideas green colorless')\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : integer\n",
    "        The number of observed sentences\n",
    "    c : float\n",
    "        The probability that 'Furiously sleep ideas green colorless' \n",
    "        is generated under L2. By default, c = .001\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The posterior odds ratio of L1 / L2 after observing n sentences\n",
    "    \n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(getPostOdds(10))\n",
    "print(getPostOdds(100))\n",
    "print(getPostOdds(1000))\n",
    "print(getPostOdds(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b6886ab745faca6e01e76f07ebf9423",
     "grade": true,
     "grade_id": "test_getPostOdds",
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_allclose\n",
    "\n",
    "assert_allclose(getPostOdds(10, c=.01), 1.1057273553218807)\n",
    "assert_allclose(getPostOdds(100, c=.05), 168.90381970677726)\n",
    "assert_allclose(getPostOdds(1000, c=.005), 150.28625220462635)\n",
    "assert_allclose(getPostOdds(1, c=.005), 1.0050251256281406)\n",
    "assert_allclose(getPostOdds(10, c=.3), 35.40133174641438)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part D (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Look at the posterior odds ratio after 10, 100, 1000, and 10000 sentences. Which lanugage were these sentences more likely to be drawn from? How does the posterior odds ratio change with the observation of additional sentences?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5f7bb0eae13fb164aed6f8cd57268655",
     "grade": false,
     "grade_id": "print_getPostOdds",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(getPostOdds(10))\n",
    "print(getPostOdds(100))\n",
    "print(getPostOdds(1000))\n",
    "print(getPostOdds(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7c7389713e3839891cc99f39d45769f0",
     "grade": true,
     "grade_id": "posteriorOddsChange",
     "points": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part E (2 points)\n",
    "\n",
    "This is just one example of how the assumption that a language is a probability distribution and learners are provided with samples from that distribution can make it possible to learn the correct language in cases where other approaches fail. In contrast to other analyses of learnability, it is possible to show that under these assumptions even complex languages (such as those represented by probabilistic context free grammars) can be learned from sufficiently large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Briefly discuss how the probabilistic approach above relates to the poverty of the stimulus argument. In particular, how does this approach handle the subset problem? How does this differ from other formal approaches to learnability (e.g., Gold's Game)?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "8cbd6be38f878925a382de1136b07695",
     "grade": true,
     "grade_id": "implicationsForLearnability",
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your exam.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
