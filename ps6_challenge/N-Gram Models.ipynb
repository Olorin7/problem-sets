{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Change the following variable to `True` if you want us to grade this challenge problem. \n",
    "\n",
    "**IMPORTANT**: You can only get points for _one_ challenge problem per problem set. This means that even if you complete both challenge problems on this problem set, we will only count your best score between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "SUBMIT_CHALLENGE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "One way to add a non-deterministic element to a chatbot is to include a language model. In this context, a language model is a probability function $p$ that assigns probabilities to particular word sequences. For example, a language model might assign a probability of 0.15 to the phrase ${\\bf w_1} = [{\\tt I}, {\\tt love}, {\\tt Comp}, {\\tt Models}]$. We can use language models to estimate whether one phrase is more plausible than another. For example, the same language model might assign a probability of 0.22 to the phrase ${\\bf w_2} = [{\\tt I}, {\\tt love}, {\\tt ice}, {\\tt cream}]$, indicating that, with no additional evidence, we are more likely to encounter ${\\bf w}_2$ than ${\\bf w}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false
   },
   "source": [
    "### N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "One simple type of language model is an **n-gram model**. In these models, the probability of the next word in a sequence is calculated using the previous $n-1$ words as context. For example, if the first three words in a sentence are\n",
    "\n",
    "$$\\texttt{Turn, in, your}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "a 4-gram language model might generate the following predictions:\n",
    "\n",
    "$$P({\\tt homework}\\  | \\ {\\tt turn}, {\\tt in}, {\\tt your}) = 0.45$$\n",
    "$$P({\\tt assignment} \\ | \\ {\\tt turn}, {\\tt in}, {\\tt your}) = 0.35$$\n",
    "$$P({\\tt badge} \\ | \\ {\\tt turn}, {\\tt in}, {\\tt your}) = 0.05$$\n",
    "$$P({\\tt grave} \\ | \\ {\\tt turn}, {\\tt in}, {\\tt your}) = 0.15$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The probability of a particular sequence of word tokens in an n-gram can be calculated using raw token frequences. Given a training corpus (i.e., a collection of texts), the probability of a word given the $n-1$ preceding tokens is simply\n",
    "\n",
    "$$P(w_n | w_1, \\ldots, w_{n-1} ) = \\frac{C(w_1, \\ldots, w_n)}{C(w_1, \\ldots, w_{n-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "where $C(w_1, \\ldots w_n)$ denotes the number of times the n-gram sequence $w_1, w_2, \\ldots, w_n$ was observed in the training corpus. Note that the above definition corresponds to the **maximum likelihood estimate (MLE)** for the training data n-grams. Under this model, the probability of generating a particular sentence consisting of $k$ words (for $k > n$) is given by\n",
    "\n",
    "$$ P(w_1, \\ldots, w_k) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3|w_1, w_2) \\times \\ldots \\times P(w_n|w_1, \\ldots w_{n-1}) \\times \\ldots P(w_k|w_{k-n-1}, \\ldots, w_{k-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "For this challenge problem you will be building an N-Gram model for use within, e.g., a chatbot. You will interact with your model using the following functions:\n",
    "1. `log_sentence_prob`: takes a complete sentence and returns its log probability according to your n-gram model. The input to this function might be something like `np.array([\"this\", \"is\", \"a\", \"complete\", \"sentence\"])`. \n",
    "2. `generate`: generates a random sentence using your model. \n",
    "\n",
    "It is up to you to select a corpus of texts that you would like to train your model on. A good starting place might be one of the texts from Project Gutenberg (e.g., [Jane Eyre](https://www.gutenberg.org/cache/epub/1260/pg1260.txt)). \n",
    "\n",
    "* While more sophisticated NLP approaches often perform significant preprocessing on a text before constructing a language model, the current assignment does not require that you do this. However, if you are interested, the following [lecture notes](http://pages.cs.wisc.edu/~jerryzhu/cs769/text_preprocessing.pdf) describe a few common techniques that may help to improve your model.\n",
    "\n",
    "Constructing your model will generally entail \n",
    "1. Cleaning up your corpus to remove extraneous formatting + punctuation + unwanted tokens\n",
    "2. Counting the frequencies of each unique sequence of $n$ word tokens in your corpus\n",
    "3. Using these sequence frequencies ($n$-grams) to compute the probability of that sequence within your corpus\n",
    "4. Using these sequence probabilities to construct a sentence by sequentially generating the most probable word given the previous $n$ words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# load your corpus\n",
    "import requests\n",
    "target_url = 'Replace this with the link to your corpus!'\n",
    "text = requests.get(target_url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1af45116bac3ccd5253be0bf81d4c40e",
     "grade": true,
     "grade_id": "challenge_1",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Given a text, returns a list consisting solely of the word \n",
    "    tokens contained therein. Feel free to write helper \n",
    "    functions for removing formatting and punctuation, converting \n",
    "    word case, creating equivalence classes, etc. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: string\n",
    "        A string containing the raw text of your corpus\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    cleaned_text: list of strings\n",
    "        a list of the words in your corpus presented in the same \n",
    "        order as they appear in text\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "def calc_ngram_probs(cleaned_text, n=3):\n",
    "    \"\"\"\n",
    "    Calculates the log probability of each unique n-word sequence \n",
    "    within a text. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cleaned_text: string\n",
    "        A list of the words in your corpus presented in the same \n",
    "        order as they appear in text\n",
    "    \n",
    "    n: int\n",
    "        The gram-size for your model (i.e., the number of words in \n",
    "        your n-gram sequences).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ngram_probs: dict\n",
    "        A dictionary of key,value pairs where the keys correspond \n",
    "        to unique n-gram word sequences and the values correspond \n",
    "        to the log probability of the sequence occurring within \n",
    "        your corpus\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "def generate(ngram_probs, seed):\n",
    "    \"\"\"\n",
    "    Completes a sentence using the probabilities using your ngram model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ngram_probs: dict\n",
    "        A dictionary of key,value pairs where keys correspond \n",
    "        to n-gram word sequences and values correspond to the \n",
    "        log probability of the sequence occurring within your \n",
    "        corpus\n",
    "    \n",
    "    seed: list\n",
    "        A list containing the first n words of a sentence.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence: list\n",
    "        A list of length >= n+1 word tokens constituting a sentence. The \n",
    "        n+1st to the last word should be generated using the probabilities\n",
    "        of your ngram model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "def log_sentence_prob(ngram_probs, sentence):\n",
    "    \"\"\"\n",
    "    Returns the log probability of a sentence using the probabilites\n",
    "    of your n-gram model. You may assume that the probability of generating \n",
    "    the first n-1 words (i.e., the seed words) is 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ngram_probs: dict\n",
    "        A dictionary of key,value pairs where keys correspond \n",
    "        to n-gram word sequences and values correspond to the \n",
    "        log probability of the sequence occurring within your \n",
    "        corpus\n",
    "    \n",
    "    sentence: list\n",
    "        A list of word tokens constituting a single sentence\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    prob: float\n",
    "        The log probability of generating the words in sentence\n",
    "        according to your n-gram model.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Test the model for n=3 (tri-grams)\n",
    "n = 3\n",
    "seed = [\"Today\", \"I\", \"went\"]\n",
    "\n",
    "cleaned_text = preprocess(text)\n",
    "ngram_probs = calc_ngram_probs(cleaned_text, n)\n",
    "sentence = generate(ngram_probs, seed)\n",
    "log_prob = log_sentence_prob(ngram_probs, sentence)\n",
    "\n",
    "print('Sentence: {}'.format(sentence))\n",
    "print('log P(Sentence): {}'.format(log_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
