{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from ipywidgets import interact, interactive\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Bayesian statistics on a continuous probability distribution\n",
    "\n",
    "In this notebook, we use what we have learned about continuous probability densities and Bayesian probability theory to model our beliefs about the <i>bias</i> of a coin: that is, the probability $\\theta$ that the coin will land heads.\n",
    "\n",
    "Recall that in order to update our beliefs, we follow Bayes' rule:\n",
    "\n",
    "$$ P(h|d)=\\frac{P(d|h)\\cdot P(h)}{P(d)}$$\n",
    "\n",
    "Here, our hypothesis is about $\\theta$, the (continuous-valued) <i>bias</i> of the coin, based on our prior beliefs about the behaviour of coins, and the outcome of coin flips we subsequently observe. Bayes' rule therefore becomes:\n",
    "\n",
    "$$ p(\\theta|d)=\\frac{p(d|\\theta)\\cdot p(\\theta)}{p(d)}$$\n",
    "\n",
    "We seek to find the value of $\\theta$ that <i>maximizes</i> this <i>posterior probability density</i>: \n",
    "$$argmax_{\\theta} \\, p(\\theta|d)$$. \n",
    "\n",
    "By examining the equation, we see that our candidate should maximize the likelihood function, $p(d|\\theta)$, and our prior, $p(\\theta)$. The name of this estimator is, therefore, the <i>maximum a posteriori</i> (MAP) estimator, as discussed in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "### Prior probability density\n",
    "\n",
    "We now need to find a functional form for our prior probability, $p(\\theta)$, which represents our baseline beliefs about the behaviour of coins. Since the value for $\\theta$ can only be between 0 and 1, we use a $Beta$ distribution to model it, as this has many nice properties. Our initial prior distribution is plotted by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Beta distribution pdf evaluated at value(s) theta\n",
    "def prior(theta, prior_tails, prior_heads):    \n",
    "    return beta.pdf(theta, prior_heads + 1, prior_tails + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# This function calculates the plots the prior distribution!\n",
    "def plot_prior(prior_tails, prior_heads):\n",
    "    \n",
    "    x = np.arange(0,1,0.01)\n",
    "    y = prior(x, prior_tails, prior_heads)\n",
    "    \n",
    "    plt.figure(1, figsize=(14,6))\n",
    "    \n",
    "    plt.plot(x, y, color='k')\n",
    "    plt.xlabel('theta')\n",
    "    plt.ylabel('Prior probability = P(theta)')\n",
    "    plt.title(\"Prior distribution over theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "w = interactive(plot_prior, prior_tails=(0,10), prior_heads=(0,10))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The graph above shows our ***prior beliefs*** about the probability, $\\theta$, the coin flip turns up heads. For mathematical convenience we use a Beta distribution, which can be shown to accurate reflect our belief about $\\theta$ given that we have observed a certain number of heads and tails before we start our experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Likelihood function\n",
    "\n",
    "We also need to model our data distribution given each candidate of $\\theta$. That is, we seek a likelihood function, $p(d|\\theta)$, that accuractely describes the density of coin-flip outcomes, in terms of heads and tails, if a coin is biased according to that particular value of $\\theta$.\n",
    "\n",
    "As this represents the outcome of $n$ independent and identically distributed $Bernoulli$ trials, the relevant probability distribution is therefore the $Binomial$:\n",
    "\n",
    "$$\\binom {n} {k} \\, \\theta^k \\, (1-\\theta)^{(n-k)}$$\n",
    "\n",
    "In other words, when considering our data of $n$ flips, the chance that $k$ of the flips landed heads is given by the product of the individual $Bernoulli$ trials that would be needed to give that number of heads $and$ tails, multiplied by the number of different ways to arrange these outcomes. \n",
    "\n",
    "For more information on the Binomial distribution, see https://en.wikipedia.org/wiki/Binomial_distribution.\n",
    "\n",
    "The code below will use these two distributions and Bayes' rule to plot the posterior density, and find the MAP estimate of $\\theta$. Try altering each of the parameters, to get a sense of how the distribution and MAP change, and how the MAP compares to the posterior mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Computes the beta-binomial posterior model pdf evaluated at value(s) theta\n",
    "def posterior(theta, num_heads, num_tails, prior_heads, prior_tails):\n",
    "    return beta.pdf(theta, prior_heads + num_heads + 1, num_tails + prior_tails + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# This function calculates the plots the posterior distribution\n",
    "def plot_prior(num_heads, num_tails, prior_heads, prior_tails):\n",
    "        \n",
    "    x = np.arange(0,1,0.01)\n",
    "    y = posterior(x, num_heads, num_tails, prior_heads, prior_tails)\n",
    "    \n",
    "    plt.figure(1, figsize=(14,5))\n",
    "    \n",
    "    plt.plot(x,y, color='k')\n",
    "    plt.xlabel('theta')\n",
    "    plt.ylabel('Posterior probability = P(theta | sequence)')\n",
    "    plt.title('Posterior distribution over theta')\n",
    "    \n",
    "    map_estimator = (prior_heads + num_heads) / (prior_tails + prior_heads + num_heads + num_tails)\n",
    "    pm_estimator = (prior_heads + num_heads + 1) / (prior_tails + prior_heads + num_heads + num_tails + 2)\n",
    "    \n",
    "    map_line = plt.axvline(map_estimator, color='r', label='MAP')\n",
    "    pm_line = plt.axvline(pm_estimator, color='b', label='Posterior Mean')\n",
    "    \n",
    "    plt.legend(handles=[map_line, pm_line])\n",
    "    \n",
    "    return {\n",
    "        'MAP': map_estimator,\n",
    "        'Posterior Mean': pm_estimator\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "interactive(plot_prior, num_heads=(0,30), num_tails=(0,30), prior_heads=(0,10), prior_tails=(0,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Outcome\n",
    "\n",
    "And we're done - we have an estimate of the most likely value of $\\theta$, according to our prior beliefs and data!\n",
    "\n",
    "For further insight into the choice of probability distributions and densities above, think about the probability density of the posterior. Is is similar to the prior of likelilihood function? What properties of this pairing ensure it will have such a shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
