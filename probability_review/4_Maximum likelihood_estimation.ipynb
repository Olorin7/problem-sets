{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<h1 style=\"margin-top:0\">Maximum-likelihood estimation</h1>\n",
    "<br/>\n",
    "<b>Maximum-likelihood estimation (MLE)</b> is a method of estimating the parameters of a statistical model given some data.\n",
    "\n",
    "For example, we might be interested in the heights of students at Berkeley. Since we can't measure the heights of the entire population, we will need to use a sample. Assuming that the heights come from a normal distribution with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE accomplishes this by finding values for the paramters (mean and variance in this case) that <em>make the observed results the most probable given the model</em>.\n",
    "\n",
    "Following previous examples in class, let's focus on a particular model that describes the behavior of a coin. A fair coin would have 0.5 probability of landing on heads, and a 0.5 probability of landing on tails. However, a coin can also be biased. We can capture this bias by finding the probability that each side of the coin will appear after being flipped. For example, we might have a 0.3 probability of gettings heads, and 0.7 for tails. This type of two-outcome distribution is called a **Bernoulli distribution**. This distribution only has one parameter $\\theta$ (\"theta\"), since knowing the probability of heads will also tell us the probability of tails ($1-\\theta$). So, our task will be to find the MLE of $\\theta$ given some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<b>Calculating the Likelihood for a Series of Coin Flips</b>\n",
    "<br>\n",
    "Before we find the maximum likelihood estimate of $\\theta$, we first need to know how to calculate how likely our data is given some arbitrary $\\theta$. The likelihood of our data is simply the product of the probabilities of each datapoint (the joint probability). Recall that using our Bernoulli distribution, we will get heads with probability $\\theta$, and tails with probability $1-\\theta$. Mathematically, this corresponds to\n",
    "\n",
    "$$P(d|\\theta) = \\theta^{N_H} \\times (1-\\theta)^{N_T} $$\n",
    "\n",
    "where $d$ is the observed sequence of coin flips, $N_H$ is the number of Heads in the sequence, and $N_T$ is the number of tails in the sequence.\n",
    "\n",
    "For example, if we set $\\theta = 0.2$, the likelihood $P(d|\\theta)$ of the sequence `T,H,T,T` will be $0.2 \\times (1-0.2)^3 = 0.1024$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "### Coding Practice\n",
    "\n",
    "Now, let's play around with come code for practice. First, let's create some datasets to play with. For work in Python, we'll denote `H`'s with a $1$ and `T`'s with a $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# slightly more heads than tails\n",
    "data_1 = np.array([1,1,0,1,0,0,1,1,1,0])\n",
    "\n",
    "# way more heads than tails\n",
    "data_2 = np.array([1,1,0,1,1,1,0,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Second, we can define a function that returns the likelihood for a sequence of data. Note that this function is simply implementing the equation we wrote down earlier for $P(d|\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def likelihood(data,theta):\n",
    "    num_heads = np.sum(data)\n",
    "    num_tails = np.sum(1-data)\n",
    "    return (theta**num_heads) * ((1-theta)**num_tails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "We can evaluate this function on both datasets assuming $\\theta = 0.6$ (i.e., that our coin was $60\\%$ likely to produce heads on any given flip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print('P(data_1 | theta=0.6): {}'.format(likelihood(data_1,0.6)))\n",
    "print('P(data_2 | theta=0.6): {}'.format(likelihood(data_2,0.6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Next, we can visualize the likelihood of the data as function many different possible values of theta between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# create 100 equally spaced values between 0 and 1\n",
    "thetas = np.linspace(0,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# take a quick look\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# compute the likelihood for each value of theta\n",
    "likelihoods_1 = [likelihood(data_1, theta) for theta in thetas] # dataset 1\n",
    "likelihoods_2 = [likelihood(data_2, theta) for theta in thetas] # dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# plot likelihood as a function of theta for both datasets\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(thetas,likelihoods_1, c='blue', label='dataset 1')\n",
    "plt.plot(thetas,likelihoods_2, c='green', label='dataset 2')\n",
    "plt.title(\"Likelihood as a function of theta\")\n",
    "plt.ylabel(\"P(d|theta)\")\n",
    "plt.xlabel(\"theta\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The **maximum likelihood estimate (MLE)** of $\\theta$ is the value that corresponds to the maximum likelihood of the data. Let's find this value by searching through the likelihoods we generated for the first dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# find the maximum likelihood value for each curve \n",
    "max_likelihood_1 = np.max(likelihoods_1)\n",
    "max_likelihood_2 = np.max(likelihoods_2)\n",
    "\n",
    "# find the values of thetas that corresponds to the maximum likelihood value\n",
    "# for each curve\n",
    "mle_1 = thetas[np.where(likelihoods_1 == max_likelihood_1)][0]\n",
    "mle_2 = thetas[np.where(likelihoods_2 == max_likelihood_2)][0]\n",
    "\n",
    "print('Maximum Likelihood Estimate for Theta under Dataset 1: {}'.format(mle_1))\n",
    "print('Maximum Likelihood Estimate for Theta under Dataset 2: {}'.format(mle_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Let's visualize these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.plot(thetas,likelihoods_1,c='blue')\n",
    "max_likelihood_1 = np.max(likelihoods_1)\n",
    "mle_1 = thetas[np.where(likelihoods_1==max_likelihood_1)][0]\n",
    "plt.plot(mle_1, max_likelihood_1,c='blue',marker='o')\n",
    "plt.axvline(x=mle_1,c='blue')\n",
    "\n",
    "plt.plot(thetas,likelihoods_2,c='green')\n",
    "max_likelihood_2 = np.max(likelihoods_2)\n",
    "mle_2 = thetas[np.where(likelihoods_2==max_likelihood_2)][0]\n",
    "plt.plot(mle_2, max_likelihood_2,c='green',marker='o')\n",
    "plt.axvline(x=mle_2,c='green')\n",
    "\n",
    "plt.title(\"Likelihood as a function of theta\")\n",
    "plt.ylabel(\"Likelihood\")\n",
    "plt.xlabel(\"theta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "From the 100 values of $\\theta$ that we evaluated earlier, we were able to find a single value that make the data most likely. However, we were only able to search 100 values for the best one, while there are actually infinitely many possible values that $\\theta$ could take between 0 and 1. This means that the value we found may be a little off, which isn't good. Luckily, with a bit of math, it can be shown that $MLE(\\theta) = \\frac{N_H}{N_H + N_T}$. Let's try this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# function to calculate MLE of theta\n",
    "def calc_mle(data):  \n",
    "    num_heads = np.sum(data)\n",
    "    num_tails = np.sum(1-data)\n",
    "\n",
    "    return num_heads / float((num_heads+num_tails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "print('Analytical MLE for Dataset 1: {}'.format(calc_mle(data_1)))\n",
    "print('Analytical MLE for Dataset 2: {}'.format(calc_mle(data_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
