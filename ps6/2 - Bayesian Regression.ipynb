{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bayesian_regression import print_equation\n",
    "from ipywidgets import interact, IntSlider, FloatSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "The priors used in Bayesian inference provide a method for controlling the flexibility of learning algorithms. In Problem Set 5, you looked at a crude way of controlling flexibility for polynomial regression: constraining the set of functions from which the algorithm was allowed to choose. In Bayesian inference, controls on flexibility are implemented through the choice of a prior over functions.\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "In this problem, the functions we'll be working with are the 10-th order polynomials:\n",
    "\n",
    "$$\n",
    "g(x) = \\theta_{10} x^{10} + \\ldots + \\theta_{1} x + \\theta_0\n",
    "$$\n",
    "\n",
    "These functions are entirely characterized by the coefficients $\\theta = (\\theta_{0}, \\ldots, \\theta_{10})$. We can define a prior over these functions by defining a distribution over $\\theta$. We will be assuming that each $\\theta_j$ follows a Gaussian (i.e. normal) distribution, so\n",
    "\n",
    "$$\n",
    "p(\\theta_j) = \\frac{1}{\\sqrt{2\\pi\\sigma_j^2}} \\exp\\left(\\frac{-\\theta_j^2}{2\\sigma_j^2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma_j = \\alpha\\beta^j$ (and $0 < \\beta \\leq 1$), and that the $\\theta_j$ are independent, so $p(\\theta) = \\prod_{j=0}^{10} p(\\theta_j)$.\n",
    "\n",
    "The function `sample_theta` samples a vector $\\theta$ from prior distribution defined above, given values of $\\alpha$, $\\beta$, and $k$ (which we will always just use at its default value, $k=10$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4adb593c61ed54cf24568cb512eae920",
     "grade": false,
     "grade_id": "sample_theta",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_theta(alpha, beta, k=10):\n",
    "    \"\"\"Sample a θ vector with degree k.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha, beta : float\n",
    "        Parameters that define sigma_j\n",
    "    k : int (default=10)\n",
    "        The order of the polynomial to sample\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array with shape (k+1,) where the first element corresponds \n",
    "    to the highest order term (x^k) and the (k+1)^th element corresponds\n",
    "    to the lowest order term (the constant).\n",
    "    \n",
    "    \"\"\"\n",
    "    theta = np.empty(k + 1)\n",
    "    for j in range(k + 1):\n",
    "        sigma_j = alpha * beta ** j\n",
    "        theta[k - j] = np.random.normal(0, sigma_j)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Calling `sample_theta` produces the $\\theta$ vector, where the first value in the vector corresponds to $\\theta_0$, and the last corresponds to $\\theta_{10}$. To help you better visualize what the $\\theta$ vector represents, we've provided you with a helper function, `print_equation`, that takes as input the $\\theta$ vector and displays a properly formatted LaTeX equation.\n",
    "\n",
    "So, we can first sample a value for $\\theta$, and then display what the corresponding equation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "theta = sample_theta(alpha=5, beta=0.5)\n",
    "\n",
    "print(\"theta: {}\".format(theta))\n",
    "print_equation(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Notice that as you run the above cell multiple times, the coefficients for $x^{10}$ and $x^9$ are frequently very close to zero. If this happens, then the polynomial is effectively a 8th or 9th order polynomial rather than a 10th order polynomial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part A (1.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Having the LaTeX rendering of our equation is nice, but it would be even nicer if we could see the line that that equation corresponds to on a graph. Complete the function `sample_theta_and_plot` to sample a value for $\\theta$ and then the corresponding function $g(x)$ for a range of $x$ values (see docstring for details).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "11548ea7a83627718dd3692c311e60da",
     "grade": false,
     "grade_id": "sample_theta_and_plot",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_theta_and_plot(axis, alpha, beta):\n",
    "    \"\"\"Samples a value for θ given α and β (and k=10),\n",
    "    and plots the corresponding polynomial. Your plot should:\n",
    "    \n",
    "    * use an array of 150 values spaced evenly between -1 and 1 \n",
    "      (including -1 and 1) as the x-coordinate values (hint: you\n",
    "      may find the np.linspace function useful here)\n",
    "    * have a single line, plotted in green, with a line width of 2\n",
    "    * return the value of theta that you sampled\n",
    "    \n",
    "    Hint: remember that you can use the np.polyval function to\n",
    "    compute y from x and the coefficients.\n",
    "    \n",
    "    Your answer can be done in 5 lines of code, including the\n",
    "    return statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : matplotlib axis object\n",
    "        The axis on which to plot the sampled line\n",
    "    alpha, beta : float\n",
    "        Parameters to pass to the `sample_theta` function\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    the sampled value of theta\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots()\n",
    "theta = sample_theta_and_plot(axis, 5, 0.5)\n",
    "print_equation(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "20026784a82be6a7ad202e5b4c84f9e3",
     "grade": true,
     "grade_id": "test_sample_theta_and_plot",
     "points": 0.5
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that sample_theta_and_plot is correct.\"\"\"\n",
    "from numpy.testing import assert_array_equal, assert_allclose\n",
    "from plotchecker import get_data\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# set the random seed and check that `sample_theta` is called correctly\n",
    "np.random.seed(10)\n",
    "fig, ax = plt.subplots()\n",
    "expected1 = np.array([\n",
    "     2.114386e-03,  -1.705080e-03,   8.381701e-05,   4.240177e-03,\n",
    "     2.074309e-02,  -1.125134e-01,   1.941675e-01,  -5.239906e-03,\n",
    "    -1.931750e+00,   1.788197e+00,   6.657933e+00])\n",
    "assert_allclose(sample_theta_and_plot(ax, 5, 0.5), expected1, rtol=1e-6)\n",
    "plt.close()\n",
    "fig, ax = plt.subplots()\n",
    "expected2 = np.array([\n",
    "    -2.058888e-05,  -7.785525e-05,  -1.416920e-04,   6.493365e-04,\n",
    "     1.970296e-04,  -5.523887e-03,   7.211229e-03,   1.234603e-02,\n",
    "     1.850893e-01,  -5.790394e-01,   2.406075e+00])\n",
    "assert_allclose(sample_theta_and_plot(ax, 2, 0.3), expected2, rtol=1e-6)\n",
    "plt.close()\n",
    "\n",
    "# check that the x and y values are plotted correctly\n",
    "fig, ax = plt.subplots()\n",
    "theta = sample_theta_and_plot(ax, 5, 0.5)\n",
    "assert_array_equal(get_data(ax)[:, 0], np.linspace(-1, 1, 150))\n",
    "assert_allclose(get_data(ax)[:, 1], np.polyval(theta, np.linspace(-1, 1, 150)))\n",
    "\n",
    "# check linestyle and color\n",
    "assert ax.lines[0].get_color() in ('g', 'green', '#008000', '#008')\n",
    "assert_equal(ax.lines[0].get_linestyle(), '-')\n",
    "\n",
    "plt.close()\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Now that you've implemented a function to sample different coefficients and plot the resulting line, we can plug your function into an IPython widget to help us more easily visualize what happens when we change the parameter values of $\\alpha$, $\\beta$, and $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7481f99f22c845d50830f6e40d9d8ab2",
     "grade": false,
     "grade_id": "prior_samples",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "alpha_slider = FloatSlider(min=0.1, max=1, step=0.1, value=0.5)\n",
    "beta_slider = FloatSlider(min=0, max=1, step=0.01, value=0)\n",
    "\n",
    "@interact\n",
    "def prior_samples(alpha=alpha_slider, beta=beta_slider):\n",
    "    # beta is strictly greater than 0, so just make it small\n",
    "    if beta == 0:\n",
    "        beta = 0.001\n",
    "\n",
    "    fig, axis = plt.subplots()\n",
    "    for i in range(20):\n",
    "        sample_theta_and_plot(axis, alpha, beta)\n",
    "    axis.set_xlim(-1, 1)\n",
    "    axis.set_ylim(-1, 1)\n",
    "    axis.set_title(r\"$\\alpha={}$, $\\beta={}$\".format(alpha, beta), fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Try changing the sliders for $\\alpha$ and $\\beta$ to see what kinds of functions get sampled. How does changing $\\alpha$ affect what functions are sampled from the prior?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "28e8784ba88873e6be71b58615e41e8b",
     "grade": true,
     "grade_id": "part_a_alpha",
     "points": 0.25,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "How does changing $\\beta$ affect what functions are sampled from the prior? Explain *why* this happens in terms of how the value of $\\beta$ affects $\\sigma_j$ and the $\\theta$ coefficients.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4638dd53eb46315736a570c572e9bbe5",
     "grade": true,
     "grade_id": "part_a_beta",
     "points": 0.75,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part B (1 point)\n",
    "\n",
    "Our data, $d$, will be vectors ${\\bf x} = (x_1, x_2, \\ldots, x_n)$ and ${\\bf y} = (y_1, y_2, \\ldots, y_n)$. We will use a Gaussian likelihood, assuming that the probability of a value of $\\mathbf{y}$ decreases exponentially as the squared distance between $y_i$ and $g_{\\theta}(x_i)$ increases,\n",
    "\n",
    "$$\n",
    "p(y_i\\ |\\ x_i,\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_Y} \\exp\\left(\\frac{-(y_i-g_{\\theta}(x_i))^2}{2\\sigma_Y^2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma_Y$ encodes how much we expect $y_i$ to vary from $g(x_i)$. Assuming that the $y_i$ are independent conditioned on $x_i$ and $\\theta$, we have\n",
    "\n",
    "$$\n",
    "p(d\\ |\\ \\theta) = \\left ( \\frac{1}{\\sqrt{2\\pi}\\sigma_Y} \\right )^n \\exp\\left(\\frac{-\\sum_{i=1}^n (y_i-g_{\\theta}(x_i))^2}{2\\sigma_Y^2}\\right)\n",
    "$$\n",
    "\n",
    "as our likelihood.\n",
    "\n",
    "We will be choosing $\\theta$ by maximum *a posteriori* (MAP) estimation, taking the value that maximizes $p(\\theta\\ | \\ d)$. Since $p(\\theta\\ |\\ d)$ is proportional to $p(d\\ |\\ \\theta)p(\\theta)$ (by Bayes' rule), we want the function that maximizes $p(d\\ |\\ \\theta)p(\\theta)$. Rather than maximizing this quantity directly, we will instead try to maximize the *log*, i.e.:\n",
    "\n",
    "$$\n",
    "\\theta_{MAP}=\\arg\\max_\\theta \\left[\\log p(d\\ |\\ \\theta) + \\log p(\\theta)\\right]\n",
    "$$\n",
    "\n",
    "From our choice of likelihood, this becomes:\n",
    "\n",
    "$$\n",
    "\\theta_{MAP}=\\arg\\max_\\theta \\left[\\frac{-1}{2\\sigma_Y^2} \\left(\\sum_{i=1}^n (y_i - g_{\\theta}(x_i))^2\\right) + \\log p(\\theta) + c\\right]\n",
    "$$\n",
    "\n",
    "where $c$ is a constant determined by $\\sigma_Y$. The first term is just the MSE on the training data multiplied by $-1/(2\\sigma_Y^2)$. A function with a high MSE on the training data can still have high posterior probability, if its prior probability is sufficiently high. The MAP estimate will thus be a function that compromises between fitting the training data well and having high prior probability.\n",
    "\n",
    "We have provided a function `bayes_polyfit` that finds the MAP estimate $\\theta_{MAP}$, when supplied with $\\mathbf{x}$, $\\mathbf{y}$, $k$, $\\sigma_Y$, $\\alpha$, and $\\beta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "62d42ae74df504644136105763ccaefa",
     "grade": false,
     "grade_id": "bayes_polyfit",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def bayes_polyfit(x, y, sigma_y, alpha, beta, k=10):\n",
    "    \"\"\"Finds the MAP estimate of θ assuming that both the likelihood\n",
    "    and prior distributions are normally distributed as follows:\n",
    "    \n",
    "    p(y | x, θ) = N(g_θ(x), σ_y)\n",
    "    p(θ_j) = N(0, αβ^j)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : numpy arrays with shape (n,)\n",
    "        The x and y values to fit to\n",
    "    sigma_y : float\n",
    "        The standard deviation of the likelihood\n",
    "    alpha, beta : float\n",
    "        Parameters that define the standard deviation of the prior\n",
    "    k : int (default=10)\n",
    "        The order of the polynomial to fit\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array with shape (k+1,) where the first element corresponds \n",
    "    to the constant term and the (k+1)^th element corresponds to the \n",
    "    coefficient for x^k.\n",
    "\n",
    "    \"\"\"\n",
    "    # compute a matrix of x^j\n",
    "    X = np.empty((x.size, k + 1))\n",
    "    for j in range(k + 1):\n",
    "        X[:, j] = x ** j\n",
    "\n",
    "    # compute a diagonal matrix of sigma_j\n",
    "    Sigma_j = np.diag([alpha * beta ** j for j in range(k + 1)])\n",
    "    Sigma_j_inv = np.linalg.inv(Sigma_j)\n",
    "        \n",
    "    # compute the posterior variance, which is Σ_post = (X'X + σ_y^2·(Σ_j)^-1)^-1\n",
    "    variance = np.linalg.inv(np.dot(X.T, X) + sigma_y**2 * Sigma_j_inv)\n",
    "    \n",
    "    # compute the posterior mean (which is the same as θ_MAP), which is Σ_post·X'·y\n",
    "    theta_map = np.dot(variance, np.dot(X.T, y))\n",
    "\n",
    "    return theta_map[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Using the function `bayes_polyfit`, we can compute $\\theta_{MAP}$ for any given set of data (as well as parameters $\\alpha$ and $\\beta$). We can use the same data that we did in Problem Set 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "data = np.load(\"data/xy_data.npy\")\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "For example, to fit the first 10 datapoints using `bayes_polyfit`, with $\\sigma_Y=0.05$, $\\alpha=5$, and $\\beta=0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "theta = bayes_polyfit(data[:10, 0], data[:10, 1], sigma_y=0.05, alpha=5, beta=0.5)\n",
    "print_equation(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "As before, it would be nice to actually plot this function to visualize it. Additionally, it would be nice to see the points that we were actually fitting to. Complete the function `plot_bayes_polyfit` to compute $\\theta_{MAP}$ using `bayes_polyfit` and then plot the corresponding function along with the given $\\mathbf{x}$ and $\\mathbf{y}$ points.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9b1014ea70a19120fe4b263da7513264",
     "grade": false,
     "grade_id": "plot_bayes_polyfit",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_bayes_polyfit(axis, x, y, sigma_y, alpha, beta):\n",
    "    \"\"\"Computes θ_MAP given x, y, α and β (and k=10), and plots both\n",
    "    x and y as well as the corresponding polynomial. Your plot should:\n",
    "    \n",
    "    * have a line, plotted in blue, corresponding to the fitted \n",
    "      polynomial\n",
    "    * use an array of 100 values spaced evenly between -0.2 and 1.2\n",
    "      (including -0.2 and 1.2) as the x-coordinate values for the line\n",
    "    * have black points corresponding to the x and y data\n",
    "    * return the value of theta that you computed\n",
    "    \n",
    "    Your answer can be done in 6 lines of code, including the return\n",
    "    statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : matplotlib axis object\n",
    "        The axis on which to plot the sampled line\n",
    "    x, y : numpy arrays with shape (n,)\n",
    "        The x and y values to fit to\n",
    "    sigma_y : float\n",
    "        The standard deviation of the likelihood\n",
    "    alpha, beta : float\n",
    "        Parameters that define the standard deviation of the prior\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    the computed value of θ_MAP\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# plot it\n",
    "fig, axis = plt.subplots()\n",
    "theta = plot_bayes_polyfit(axis, data[:10, 0], data[:10, 1], sigma_y=0.05, alpha=5, beta=0.5)\n",
    "print_equation(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc3a51d2e59627efa890c0b54da4c800",
     "grade": true,
     "grade_id": "test_plot_bayes_polyfit",
     "points": 0.5
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that plot_bayes_polyfit is correct.\"\"\"\n",
    "from numpy.testing import assert_array_equal, assert_allclose\n",
    "from plotchecker import get_data\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# check that `plot_bayes_polyfit` is called correctly\n",
    "fig, axis = plt.subplots()\n",
    "expected1 = np.array([\n",
    "   -0.00337945, -0.0067104 , -0.0126453 , -0.02192577, -0.03267307,\n",
    "   -0.03356233,  0.01007254,  0.16191439,  0.37717652, -0.35995902,\n",
    "    0.12198659])\n",
    "actual1 = plot_bayes_polyfit(axis, data[:10, 0], data[:10, 1], sigma_y=0.05, alpha=5, beta=0.5)\n",
    "assert_allclose(actual1, expected1, rtol=1e-6)\n",
    "plt.close()\n",
    "fig, axis = plt.subplots()\n",
    "expected2 = np.array([\n",
    "    0.59930636,  0.18540162, -0.24609423, -0.56185745, -0.58111726,\n",
    "   -0.17484288,  0.4949956 ,  0.73813432,  0.01918733, -0.3841807 ,\n",
    "    0.13539476])\n",
    "actual2 = plot_bayes_polyfit(axis, data[:20, 0], data[:20, 1], sigma_y=0.01, alpha=3, beta=0.8)\n",
    "assert_allclose(actual2, expected2, rtol=1e-6)\n",
    "plt.close()\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "theta = plot_bayes_polyfit(axis, data[:10, 0], data[:10, 1], sigma_y=0.05, alpha=5, beta=0.5)\n",
    "\n",
    "# make sure there are exactly two lines (the blue line and the black points)\n",
    "assert_equal(len(axis.lines), 2)\n",
    "if axis.lines[0].get_xydata().shape == (100, 2):\n",
    "    blue_line = axis.lines[0]\n",
    "    black_points = axis.lines[1]\n",
    "else:\n",
    "    blue_line = axis.lines[1]\n",
    "    black_points = axis.lines[0]\n",
    "\n",
    "assert_array_equal(blue_line.get_xydata()[:, 0], np.linspace(-.2, 1.2, 100))\n",
    "assert_array_equal(black_points.get_xydata()[:, 0], data[:10, 0])\n",
    "assert_allclose(blue_line.get_xydata()[:, 1], np.polyval(theta, np.linspace(-.2, 1.2, 100)))\n",
    "assert_allclose(black_points.get_xydata()[:, 1], data[:10, 1])\n",
    "\n",
    "# check linestyle and color\n",
    "assert blue_line.get_color() in ('b', 'blue', '#0000FF', '#00F', (0.0, 0.0, 1.0))\n",
    "assert black_points.get_color() in ('k', 'black', '#000000', '#000', (0.0, 0.0, 0.0))\n",
    "assert_equal(blue_line.get_linestyle(), '-')\n",
    "assert_equal(black_points.get_linestyle(), 'None')\n",
    "assert_equal(black_points.get_marker(), 'o')\n",
    "\n",
    "plt.close()\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Once your implementation of `plot_bayes_polyfit` is complete, we can plug it into another IPython widget in order to visualize the effect of changing $\\sigma_Y$, $\\alpha$, and $\\beta$ on the MAP estimate of $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "feebad9a531eb95a9521dd4641aff155",
     "grade": false,
     "grade_id": "visualize_plot_bayes_polyfit",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "sigma_y_slider = FloatSlider(min=0.0, max=1, step=0.01, value=0.05)\n",
    "alpha_slider = FloatSlider(min=0.1, max=1, step=0.1, value=0.5)\n",
    "beta_slider = FloatSlider(min=0, max=1, step=0.01, value=0)\n",
    "\n",
    "@interact\n",
    "def visualize_plot_bayes_polyfit(sigma_y=sigma_y_slider, alpha=alpha_slider, beta=beta_slider):\n",
    "    # sigma_y is strictly greater than 0, so just make it small\n",
    "    if sigma_y == 0:\n",
    "        sigma_y = 0.001\n",
    "    # beta is strictly greater than 0, so just make it small\n",
    "    if beta == 0:\n",
    "        beta = 0.001\n",
    "\n",
    "    fig, axis = plt.subplots()\n",
    "    plot_bayes_polyfit(axis, data[:10, 0], data[:10, 1], sigma_y, alpha, beta)\n",
    "    axis.set_ylim(0, 0.25)\n",
    "    axis.set_title(r\"$\\sigma_Y={}$, $\\alpha={}$, $\\beta={}$\".format(sigma_y, alpha, beta), fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\">Try moving the sliders to change the values of $\\sigma_Y$, $\\alpha$, and $\\beta$. What effect does the value of $\\sigma_Y$ have on on the functions found by this MAP procedure?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec98d28dc7f4f5647f7b55f33836012b",
     "grade": true,
     "grade_id": "part_b_sigma_y",
     "locked": false,
     "points": 0.25,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "What is the effect of the prior term on the functions found by this MAP procedure?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5c95e4ec06450f8a7b9a7fba2318e076",
     "grade": true,
     "grade_id": "part_b_prior",
     "points": 0.25,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part C (1 point)\n",
    "\n",
    "Even though we've now defined a way to compute estimates of $\\theta_{MAP}$, we still want to know whether that estimate is actually any good or not. How well does it generalize to other data? We can try to answer this question by training on one set of data, and then computing the mean squared error on a different set of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Complete the function `mse` to compute $\\theta_{MAP}$ for the training data, and then to return the mean squared error (MSE) for both the training and testing data. This function should be very similar to the MSE function you wrote in Problem Set 5.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Note: Your function should call `bayes_polyfit` EXACTLY once. You should not be calling it twice!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "935896418b49c3435caee9f8e20c52d5",
     "grade": false,
     "grade_id": "mse",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mse(train, test, sigma_y, alpha, beta):\n",
    "    \"\"\"Fits a polynomial to a training dataset using MAP estimation, and \n",
    "    then returns the mean squared error (MSE) between the y-values\n",
    "    of the training data and the fitted polynomial, and the MSE\n",
    "    between the y-values of the test data and the fitted polynomial.\n",
    "    \n",
    "    Your answer can be done in 6 lines of code, including the return\n",
    "    statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : numpy array with shape (n, 2)\n",
    "        The training data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "    test : numpy array with shape (m, 2)\n",
    "        The testing data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "    sigma_y : float\n",
    "        The standard deviation of the likelihood\n",
    "    alpha, beta : float\n",
    "        Parameters that define the standard deviation of the prior\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a 2-tuple consisting of the training set MSE and testing set MSE\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "For example, we can compute the MSE for $\\sigma_Y=0.05$, $\\alpha=5$, and $\\beta=0.5$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# compute the MSE\n",
    "train_mse, test_mse = mse(data[:10], data[10:], 0.05, 5, 0.5)\n",
    "print(\"The training error is: \" + str(train_mse))\n",
    "print(\"The testing error is:  \" + str(test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# add your own test cases here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fd83a0424f8e4d36bdb2b8c3e9cfe90f",
     "grade": true,
     "grade_id": "test_mse",
     "points": 0.5
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test that the `mse` function is correct.\"\"\"\n",
    "from numpy.testing import assert_allclose\n",
    "\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# use first ten, and the remaining\n",
    "assert_allclose(mse(data[:10], data[10:], 0.05, 5, 0.5), (0.00017923119244892278, 0.00042229320744594277))\n",
    "assert_allclose(mse(data[:10], data[10:], 0.01, 3, 0.8), (0.00013236899868029913, 0.00048738313668677277))\n",
    "assert_allclose(mse(data[:10], data[10:], 0.02, 4, 0.4), (0.00016345646919385944, 0.00044807927181970573))\n",
    "\n",
    "# use half-and-half\n",
    "assert_allclose(mse(data[:55], data[55:], 0.05, 5, 0.5), (0.00034014346736179748, 0.00039665479872906133))\n",
    "assert_allclose(mse(data[:55], data[55:], 0.01, 3, 0.8), (0.00033488020345527299, 0.00041108744745287085))\n",
    "assert_allclose(mse(data[:55], data[55:], 0.02, 4, 0.4), (0.00033859355834118019, 0.00039868682354987449))\n",
    "\n",
    "# use last twenty, and the remaining\n",
    "assert_allclose(mse(data[-20:], data[:-20], 0.05, 5, 0.5), (0.00022355555903380491, 0.00053064829628726782))\n",
    "assert_allclose(mse(data[-20:], data[:-20], 0.01, 3, 0.8), (0.00019604154213550924, 0.00055087407914624611))\n",
    "assert_allclose(mse(data[-20:], data[:-20], 0.02, 4, 0.4), (0.00022166576215126383, 0.00053418258236528535))\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2d253f4e12ddb47f95954ac943fd9fbd",
     "grade": false,
     "grade_id": "plot_mse",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_mse(axis, train, test, sigma_y=0.05, alpha=5):\n",
    "    \"\"\"Plot the mean squared error (MSE) for the given training and testing\n",
    "    data as a function of beta.\n",
    "    \n",
    "    * Your plot should show the MSE for 150 values of beta evenly spaced\n",
    "      between 0.001 and 1 (including 0.001 and 1)\n",
    "    * There should be two lines: one blue, for the training set error, and\n",
    "      one red, for the testing set error.\n",
    "    * Make sure to include labels for the x- and y- axes.\n",
    "    * Label the training error and testing error lines as \"Training set error\" \n",
    "      and \"Testing set error\", respectively. These labels will be used to\n",
    "      create a legend later on (and so you should NOT actually create the\n",
    "      legend yourself -- just label the lines). See the documentation for\n",
    "      `axis.plot` to see how to assign a label to a line.\n",
    "      \n",
    "    Your answer can be done in 9 lines of code, including the return statement.\n",
    "      \n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : matplotlib axis object\n",
    "        The axis on which to plot the MSE\n",
    "    train : numpy array with shape (n, 2)\n",
    "        The training data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "    test : numpy array with shape (m, 2)\n",
    "        The testing data, where the first column corresponds to the\n",
    "        x-values, and the second column corresponds to the y-values\n",
    "    sigma_y : float\n",
    "        The standard deviation of the likelihood\n",
    "    alpha : float\n",
    "        Parameter that partially defines| the standard deviation of the prior\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array with shape (150, 2)\n",
    "        The MSE for the training data (corresponding to the first column) and\n",
    "        for the testing data (corresponding to the second column). Each row\n",
    "        corresponds to a different value of beta.\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "After implementing the `plot_mse` function, you should be able to see the error as a function of $\\beta$ for both the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# plot it\n",
    "fig, axis = plt.subplots()\n",
    "plot_mse(axis, data[:10], data[10:])\n",
    "axis.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "856daf6bbf59711c710205783cabbb3d",
     "grade": true,
     "grade_id": "test_plot_mse",
     "points": 0.5
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Is the plot_mse function correctly implemented?\"\"\"\n",
    "from nose.tools import assert_equal, assert_not_equal\n",
    "from numpy.testing import assert_allclose\n",
    "from plotchecker import get_data\n",
    "\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "# check that it uses the mse function\n",
    "old_mse = mse\n",
    "del mse\n",
    "try:\n",
    "    fig, axis = plt.subplots()\n",
    "    plot_mse(axis, data[:10], data[10:])\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"plot_mse should call mse, but it does not\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    mse = old_mse\n",
    "    del old_mse\n",
    "    \n",
    "fig, axis = plt.subplots()\n",
    "error = plot_mse(axis, data[:10], data[10:])\n",
    "axis.legend(loc='upper left')\n",
    "\n",
    "# check the error\n",
    "assert_equal(error.shape, (150, 2))\n",
    "xvals = np.linspace(0.001, 1, 150)\n",
    "assert_allclose(error[0], mse(data[:10], data[10:], 0.05, 5, xvals[0]))\n",
    "assert_allclose(error[4], mse(data[:10], data[10:], 0.05, 5, xvals[4]))\n",
    "assert_allclose(error[8], mse(data[:10], data[10:], 0.05, 5, xvals[8]))\n",
    "assert_allclose(error[90], mse(data[:10], data[10:], 0.05, 5, xvals[90]))\n",
    "assert_allclose(error[99], mse(data[:10], data[10:], 0.05, 5, xvals[99]))\n",
    "\n",
    "# check that there are two lines\n",
    "assert_equal(len(axis.lines), 2)\n",
    "if axis.lines[0].get_color() in ('b', 'blue', '#0000FF', '#00f', (0.0, 0.0, 255.0)):\n",
    "    train_line = axis.lines[0]\n",
    "    test_line = axis.lines[1]\n",
    "else:\n",
    "    train_line = axis.lines[1]\n",
    "    test_line = axis.lines[0]\n",
    "\n",
    "# check the plotted data\n",
    "assert_allclose(train_line.get_xydata()[:, 0], np.linspace(0.001, 1, 150))\n",
    "assert_allclose(test_line.get_xydata()[:, 0], np.linspace(0.001, 1, 150))\n",
    "assert_allclose(train_line.get_xydata()[:, 1], error[:, 0])\n",
    "assert_allclose(test_line.get_xydata()[:, 1], error[:, 1])\n",
    "\n",
    "# check the line colors\n",
    "assert train_line.get_color() in ['b', 'blue', (0.0, 0.0, 255.0), '#0000FF', '#00f']\n",
    "assert test_line.get_color() in ['r', 'red', (1, 0, 0), '#FF0000']\n",
    "\n",
    "# check the legend\n",
    "legend_labels = [x.get_text() for x in axis.get_legend().get_texts()]\n",
    "assert_equal(legend_labels, [\"Training set error\", \"Testing set error\"])\n",
    "\n",
    "# check the axis labels\n",
    "assert_not_equal(axis.get_xlabel(), \"\")\n",
    "assert_not_equal(axis.get_ylabel(), \"\")\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part D (1 point)\n",
    "\n",
    "Now, we will use another IPython widget to visualize how the error changes depending on the dataset that we are fitting to. The widget will call your `plot_mse` function with different subsets of the data, depending on the index that is set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "51f94890b01a8b959491737b98329eef",
     "grade": false,
     "grade_id": "make_polynomial_fit_and_graph",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = np.load(\"data/xy_data.npy\")\n",
    "\n",
    "@interact\n",
    "def make_polynomial_fit_and_graph(training_set_index=(1, 11)):\n",
    "    # relabel the index for convenience\n",
    "    i = training_set_index\n",
    "    \n",
    "    # pull out the training and testing data\n",
    "    traindata = data[((i - 1) * 10):(i * 10)]\n",
    "    testdata = np.concatenate([data[:((i - 1) * 10)], data[(i * 10):]])\n",
    "\n",
    "    # plot the MSE\n",
    "    fig, axis = plt.subplots()\n",
    "    plot_mse(axis, traindata, testdata, 0.05, 5)\n",
    "    axis.set_ylim(0, 0.003)\n",
    "    axis.set_title(\"MSE for dataset #{}\".format(i))\n",
    "    axis.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "The MSE on the test set is an approximation to the generalization error produced by a learning algorithm. In class, we discussed how the generalization error of a learning algorithm is affected by both its bias and its variance. Both the bias and the variance depend upon the true function that is being estimated. An algorithm with high bias will systematically produce predictions that differ from the true function.  An algorithm with high variance will produce predictions that can deviate wildly depending on the specifics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">Suggest how the results from the MSE on the test set could be explained in terms of the bias and variance of the learning algorithms that result from using different priors over functions.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a8d1aaa0bc292470de0ad3bb91611045",
     "grade": true,
     "grade_id": "part_c",
     "points": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---\n",
    "## Part E (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "<div class=\"alert alert-success\">The 10th-order polynomials are a very flexible class of functions.  Why were we sometimes able to get away with using a learning algorithm that can choose from such a large set of functions without suffering from overfitting?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "561bd622b8514f51ab0176f6e1db36b5",
     "grade": true,
     "grade_id": "part_d",
     "points": 0.5,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {
    "284b22a852554538901f304a68fbf0e8": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "40c57404b7514295868f05a78ff9d2fb": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "afeccb2ef9e34a838a38dace453bfc2e": {
     "views": [
      {
       "cell_index": 46
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
