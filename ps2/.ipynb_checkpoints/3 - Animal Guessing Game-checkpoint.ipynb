{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import animal_guessing_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "In this problem you are going to play an animal guessing game. In this game, players are shown some features of a particular animal and the goal is to guess which animal has these features. The player is first shown two features of the animal, and then a list of animals to guess from. If the player guesses incorrectly, more features are shown and they will get more chances to guess the animal. If they cannot guess the animal correctly after all the features have been displayed, then they lose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part A (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Run the game by running the function `animal_guessing_game.play` below. This script will play the game for varying numbers of animals to choose from. Specifically, the game will run 15 times: displaying either 2, 4, 8, 16, or 32 animals to select an answer from, and running 3 iterations for each of these settings. The function `animal_guessing_game.play` saves the results of the game to the `data` directory upon completion in a NumPy file called `my_trial_data.npy`.\n",
    "\n",
    "This file, `my_trial_data.npy`, will contain an array of shape `(3, 5)` matrix, where the rows are iterations, and the columns are number of animals shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Uncomment the following cell and run it to play the game. When your submit your problem set, your data will be automatically included in the submission.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">Warning: the features used in this guessing game are a little weird and sometimes counterintuitive. Just do your best, even if the features seem surprising.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "my_trial_data = animal_guessing_game.play()\n",
    "my_trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c967f0d5277e302a9595955cb29a74d9",
     "grade": true,
     "grade_id": "trial_data",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that my_trial_data exists and is in the correct format.\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "data = np.load(\"data/my_trial_data.npy\")\n",
    "assert_equal(data.shape, (3, 5))\n",
    "assert_equal(data.dtype, np.int64)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Describe your experience playing the game. Did it become more difficult with more animals? (**0.25 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b91e24f9c2b8106226b7702ad7d9ab85",
     "grade": true,
     "grade_id": "guessing_game_experience_1",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Do you feel like this difficulty scaled linearly (i.e., was it twice as hard to play with twice as many animals, or was it less than twice as hard, or more than twice as hard)? (**0.25 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c5031214aa7ba390f4f743f485be819b",
     "grade": true,
     "grade_id": "guessing_game_experience_2",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part B (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Let's try to model the inferences made by players of this game using a simple program. Your program should play the game by looking at the given choices (animals) and observations (features) and ruling out possible hypotheses.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b00bed7389b78ccd9ce8560867eab83",
     "grade": false,
     "grade_id": "guess_animal",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def guess_animal(observations, animals):\n",
    "    \"\"\"Guesses an animal based on the set of allowed choices, the \n",
    "    observed features, and knowledge of which animals have which \n",
    "    features.\n",
    "    \n",
    "    Hint: this problem is very similar to the `guess_language` function in\n",
    "    Problem 2.\n",
    "    \n",
    "    Your solution can be done in 3 lines of code, including the return\n",
    "    statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observations : set of strings\n",
    "        The names of the features that we have observed\n",
    "    animals : dictionary of sets\n",
    "        A dictionary where the keys are the animals we have to\n",
    "        choose from, and where the values are a set of the features\n",
    "        belonging to that animal.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    The name of the animal which is our guess.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "For example, we can test this on a small subset of animals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "animals = {\n",
    "    'raccoon': {'grey',},\n",
    "    'grizzly bear': {'bulbous',},\n",
    "    'killer whale': {'bulbous', 'lean', 'flippers'}\n",
    "}\n",
    "\n",
    "guess_animal({'grey',}, animals) # should return 'killer whale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e9c2cb6a48b24370a5f2d87428206a92",
     "grade": true,
     "grade_id": "test_guess_animal",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check the implementation of guess_animal\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "for n in range(1, 21):\n",
    "    animals = {}\n",
    "    for i in range(n):\n",
    "        features = set(\"feature_{}\".format(j) for j, f in enumerate(np.random.randint(0, 2, 30).astype(bool)) if f)\n",
    "        animals[\"animal_{}\".format(i)] = features\n",
    "\n",
    "    target = \"animal_{}\".format(np.random.randint(0, n))\n",
    "    features = list(animals[target])\n",
    "    np.random.shuffle(features)\n",
    "\n",
    "    for j in range(1, n):\n",
    "        observations = features[:j]\n",
    "        guess = guess_animal(set(observations), animals)\n",
    "        \n",
    "        for obs in observations:\n",
    "            assert obs in animals[guess], \"The guessed animal {} does not have feature '{}'\".format(guess, obs)\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part C (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Once you have your `guess_animal` function working, you can use it to actually play the game. The `animal_guessing_game.model` function takes as an argument the guessing function, and plays the game 500 times at each of the different hypothesis sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model_trial_data = animal_guessing_game.model(guess_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "It produces an array with shape `(500, 5)`, that contains 500 model simulations for each of the 5 hypothesis sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model_trial_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "model_trial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The students who took this class last year also played the animal guessing game. We have provided you with their data in a file called `old_trial_data.npy` (which is saved in the `data` directory). This is a NumPy array with shape `(294, 5)`, corresponding to 294 repetitions of each of the 5 hypothesis sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "old_trial_data = np.load(\"data/old_trial_data.npy\")\n",
    "old_trial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">To compare this to the model data simulations we just ran, let's take the mean across repetitions and then plot it. To help us do this, we will create a generic function, `plot_trial_data`, which takes an array of trial data, computes the mean, plots it, and then returns the mean.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fe76281c0daf77f44419124f6049a682",
     "grade": false,
     "grade_id": "plot_trial_data",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_trial_data(axis, trial_data, label):\n",
    "    \"\"\"Plot the mean of the given trial data against the hypotheses\n",
    "    used in the animal guessing game (2, 4, 8, 16, and 32). Label\n",
    "    the line in the plot using the given label. Returns the computed\n",
    "    mean trial data.\n",
    "    \n",
    "    Make sure you also label your axes and give the plot a title.\n",
    "    \n",
    "    Note: to add a label to a line, use the `label` keyword argument\n",
    "    to the `plot` function.\n",
    "    \n",
    "    Hint: your solution can be done in 6 lines of code, including the\n",
    "    return statement.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : matplotlib axis object\n",
    "    trial_data : numpy array of shape (n, 5)\n",
    "        The rows correspond to repetitions of the game, and the\n",
    "        columns correspond to the different hypothesis sizes.\n",
    "    label : string\n",
    "        The label to apply to the line in the trial data plot.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    The mean of `trial_data`, which should be a numpy array with\n",
    "    shape (5,).\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "You can use your plotting function to visualize the results from your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots()\n",
    "plot_trial_data(axis, np.load(\"data/model_trial_data.npy\"), \"model's data\")\n",
    "axis.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8eac62ee0aa80dcf2fcf103c6ea2fe11",
     "grade": true,
     "grade_id": "test_plot_trial_data",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from plotchecker import get_data\n",
    "from nose.tools import assert_equal, assert_not_equal\n",
    "from numpy.testing import assert_array_equal\n",
    "\n",
    "for i in range(2):\n",
    "    # generate some random data\n",
    "    data = np.random.randint(1, 30, (np.random.randint(10, 200), 5))\n",
    "    atad = np.random.randint(1, 30, (np.random.randint(10, 200), 5))\n",
    "\n",
    "    # plot it\n",
    "    figure, axis = plt.subplots()\n",
    "    mean_data = plot_trial_data(axis, data, \"label {}\".format(i))\n",
    "    legend = axis.legend(loc='best')\n",
    "\n",
    "    # make sure the mean data is correct\n",
    "    assert_array_equal(mean_data, np.mean(data, axis=0))\n",
    "\n",
    "    # check the plot data\n",
    "    plot_data = get_data(axis)\n",
    "    assert_array_equal(plot_data, np.vstack([[2, 4, 8, 16, 32], mean_data]).T)\n",
    "\n",
    "    # check that plot uses solid lines\n",
    "    lines = axis.get_lines()[0]\n",
    "    assert_equal(lines.get_linestyle(), '-')\n",
    "    \n",
    "    # check axis labels and title\n",
    "    assert_not_equal(axis.get_xlabel(), '')\n",
    "    assert_not_equal(axis.get_ylabel(), '')\n",
    "    assert_not_equal(axis.get_title(), '')\n",
    "\n",
    "    # check the legend\n",
    "    assert legend is not None, \"line is not correctly labeled\"\n",
    "    legend_text = legend.get_texts()\n",
    "    assert_equal(len(legend_text), 1)\n",
    "    assert_equal(legend_text[0].get_text(), \"label {}\".format(i))\n",
    "    \n",
    "    # check lines have different colors\n",
    "    new_data = plot_trial_data(axis, atad, \"label {}\".format(i+2))\n",
    "    colors = [a.get_color() for a in axis.get_lines()]\n",
    "    assert_equal(len(colors), len(np.unique(colors)))\n",
    "    \n",
    "    # close the plot\n",
    "    plt.close(figure)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Once you have completed `plot_trial_data`, let's use it to plot your data, last year's data, and the model simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "308e2cbc4498f1da6efd5ab5a12861a0",
     "grade": false,
     "grade_id": "comparing_trial_data",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots()\n",
    "plot_trial_data(axis, np.load(\"data/my_trial_data.npy\"), \"my data\")\n",
    "plot_trial_data(axis, np.load(\"data/old_trial_data.npy\"), \"last year's data\")\n",
    "plot_trial_data(axis, np.load(\"data/model_trial_data.npy\"), \"model's data\")\n",
    "axis.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Does the model seem to play the game in the same way that the students from last year did? Did it play the same way that you did? Explain your answer. (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8f420f7f2bd155f516beb412e3501833",
     "grade": true,
     "grade_id": "interpret_results_1",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">If yes, do you think the model is a good account of people's inductive inferences in this type of guessing game? If not, why do you think the model behaves differently from people (that is, what is missing in the model that causes it to be a poor account)? (**0.5 points**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0b5a8a77724e05ddb2601567d6e18666",
     "grade": true,
     "grade_id": "interpret_results_2",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
