{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">In this problem, you will create one- and multi-layer neural networks and explore their capabilities. These problems use ideas introduced in the readings:\n",
    "<ol>\n",
    "<li> _AIMA3_, pages 727-731, or _AIMA2_, pages 736-744.\n",
    "<li> Elman, J., Bates, E., Karmiloff-Smith, A., Johnson, M., Parisi, D., & Plunkett, K. (1996). _Rethinking innateness: Development in a connectionist perspective._ Cambridge, MA: MIT Press. Chapter 2.\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "## Part A (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "You will first train a one-layer feedforward neural network (a\n",
    "\"perceptron\") to perform the function of the logical OR and XOR operators. The perceptron will be trained on a set of input and output data that you define. Each input variable is in the form of a row-vector of 1's and -1's, which stand for true and false. Note that this is different from the convention we have been using so far in the course: false values are represented with -1 rather than 0, and different columns correspond to different training examples. For instance, the input variable `A` of the function `A OR B` might be `[-1, -1, 1, 1]` where the first element is the value of `A` in the first training example, the second element is the value of `A` in the second training example, and so on. Likewise, the second input variable `B` and the output variable are also row-vectors of `1`s and `-1`s. Hence, your training set needs to be specified using two matrices. The input matrix has *m* columns, where *m* is the number of training examples. The output has one row of length *m*.\n",
    "\n",
    "Here is an example of an input matrix that would be appropriate in a training set for the logical AND\n",
    "operator:\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "-1 & -1 &  1 & 1 \\\\\n",
    "-1 &  1 & -1 & 1 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "The first row corresponds to the variable `A` and the second row corresponds to the variable `B` in `A AND B`.\n",
    "\n",
    "And here is the corresponding output for the training set which represents the corresponding values of `A AND B`:\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 & 1 \\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Define the training sets for the OR and XOR operators as the truth table for the logical OR operator and the logical XOR operator, respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Hint*: The example above does the same thing for the AND operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3611181b3a9830f823dc2433d4e5a7d4",
     "grade": false,
     "grade_id": "create_inputs",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_inputs():    \n",
    "    \"\"\"\n",
    "    Returns the training set inputs based on the truth table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An array with shape (2, 4).\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8cef09211267755140e947d3d96bd0cf",
     "grade": false,
     "grade_id": "create_inputs_or",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_outputs_OR():\n",
    "    \"\"\"\n",
    "    Returns the training outputs based on the truth table for the OR operator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An array with shape (4,).\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "adc6a3f233304b302138aa24c48885c6",
     "grade": false,
     "grade_id": "create_inputs_xor",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_outputs_XOR():\n",
    "    \"\"\"\n",
    "    Returns the training outputs based on the truth table for the XOR operator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An array with shape (4,).\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Let's see if the inputs and outputs are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "inputs = create_inputs()\n",
    "outputs_OR = create_outputs_OR()\n",
    "outputs_XOR = create_outputs_XOR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "108b362af60772898850c1005de8605b",
     "grade": true,
     "grade_id": "test_create_inputs",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the inputs and outputs\n",
    "inputs = create_inputs()\n",
    "outputs_OR = create_outputs_OR()\n",
    "outputs_XOR = create_outputs_XOR()\n",
    "\n",
    "# Check that inputs and outputs are the correct shapes.\n",
    "assert(inputs.shape == (2, 4))\n",
    "assert(outputs_OR.shape == (4,))\n",
    "assert(outputs_XOR.shape == (4,))\n",
    "\n",
    "# Check that inputs and outputs match the logical operators.\n",
    "inputs_bool = ((inputs + 1) / 2).astype(bool)\n",
    "outputs_OR_bool = ((outputs_OR + 1) / 2).astype(bool)\n",
    "outputs_XOR_bool = ((outputs_XOR + 1) / 2).astype(bool)\n",
    "assert((np.logical_or(inputs_bool[0, :], inputs_bool[1, :]) == outputs_OR_bool).all())\n",
    "assert((np.logical_xor(inputs_bool[0, :], inputs_bool[1, :]) == outputs_XOR_bool).all())\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "## Part B (1.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The perceptron, which is a simple one-layer neural network, can be created and trained by calling the function `train_perceptron`. This function takes three arguments:\n",
    "\n",
    "+ `X` — the training input matrix\n",
    "+ `Y` — the training output vector\n",
    "+ `bias` — a weight which you can think of as the weight for an extra input that always\n",
    "takes the value $1$ (i.e. it determines the baseline activation of the\n",
    "output when receiving no input).\n",
    "\n",
    "\n",
    "`train_perceptron` returns a tuple `(weights, beta)` — the vector of weights and the learned bias weight.   \n",
    "\n",
    "Using `inputs` and `outputs_OR` that you defined in Part A, and setting `bias = 0`, let's train a one-layer feedforward neural net and return the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from train_perceptron import train_perceptron\n",
    "(weights_OR, beta) = train_perceptron(inputs, outputs_OR, bias=0, num_loops=10, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "After training the above perceptron, these are the weights and bias that we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "weights_OR, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Recall that in this one-layer network, the output of the network is\n",
    "\\begin{align}\n",
    "y = \\text{tanh}(w_1x_1 + w_2x_2 + b)\n",
    "\\end{align}\n",
    "\n",
    "where $x_1$ and $x_2$ are the values of the inputs, $w_1$ and $w_2$\n",
    "are the values of the corresponding weights, $b$ is the bias weight. tanh() is a hyperbolic tangent function which produces positive outputs when its input is greater than 0.5 and negative outputs when its input is less than 0.5.\n",
    "\n",
    "Let's plot the $\\text{tanh}$ function to remember exactly what it does. Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000)\n",
    "plt.figure()\n",
    "plt.title(\"Visualizing the tanh function\")\n",
    "plt.plot(x, np.tanh(x), label=\"$y=tanh(x)$\")\n",
    "plt.legend(), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Implement the one-layer network equation above to compute the output of the perceptron.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Hint*: You can use `np.tanh` to compute the hyperbolic tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "46f0f4a2ed7b295653d6ebb20fed4aa2",
     "grade": false,
     "grade_id": "perceptron_output",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_output(x1, x2, weight1, weight2, bias):\n",
    "    \"\"\"Compute the output of a perceptron.\n",
    "    \n",
    "    Hint: your solution can be done in 1 line of code (including\n",
    "    the return statement).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1, x2 : input values\n",
    "    weight1, weight2: weights\n",
    "    bias: the bias\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scalar : the output of the perceptron.\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "647bb17670c305c308cc08de9c9dbb0f",
     "grade": true,
     "grade_id": "test_perceptron_output",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(perceptron_output(1, 1, 1, 1, 1), 0.995054753687)\n",
    "np.testing.assert_almost_equal(perceptron_output(0, 0, 0, 0, 0), 0)\n",
    "np.testing.assert_almost_equal(perceptron_output(-1, -1, -1, -1, -1), 0.761594155956)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Now, use `train_perceptron` and `perceptron_output` to first train the perceptron, and then compute it's final output for the given set of inputs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5172458336b04e8be8e0c2d362801725",
     "grade": false,
     "grade_id": "prediction",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def prediction(inputs, outputs):\n",
    "    \"\"\"Train a perceptron on the given inputs and outputs, and\n",
    "    return its final prediction for the inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : numpy array\n",
    "        A numpy array with shape (2,n) containing -1 and 1\n",
    "    outputs : numpy array\n",
    "        A numpy array with shape (n,) containing -1 and 1\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array\n",
    "        A numpy array with shape (n,) corresponding to the network\n",
    "        predictions for each input.\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# add your own test cases here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c569f2547a2e5028e858ddff0d6e2f0b",
     "grade": true,
     "grade_id": "test_prediction",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# create inputs and outputs\n",
    "inputs = create_inputs()\n",
    "outputs_OR = create_outputs_OR()\n",
    "outputs_XOR = create_outputs_XOR()\n",
    "\n",
    "# compute the prediction for OR\n",
    "out = prediction(inputs, outputs_OR)\n",
    "\n",
    "# Check that it is the correct shape.\n",
    "assert(out.shape == (4,))\n",
    "\n",
    "# Check that values are correct.\n",
    "np.testing.assert_almost_equal(sum(out), 1.99529701363)\n",
    "np.testing.assert_almost_equal(np.prod(out), -0.985958595232)\n",
    "\n",
    "# compute the prediction for XOR\n",
    "out = prediction(inputs, outputs_XOR)\n",
    "\n",
    "# Check that it is the correct shape.\n",
    "assert(out.shape == (4,))\n",
    "\n",
    "# Check that values are correct.\n",
    "np.testing.assert_almost_equal(sum(out), 0)\n",
    "np.testing.assert_almost_equal(np.prod(out), 0.840547830147)\n",
    "\n",
    "# Check that `train_perceptron` was used\n",
    "old_train_perceptron = train_perceptron\n",
    "del train_perceptron\n",
    "try:\n",
    "    prediction(inputs, outputs_OR)\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"prediction does not call the train_perceptron function\")\n",
    "finally:\n",
    "    train_perceptron = old_train_perceptron\n",
    "    del old_train_perceptron\n",
    "\n",
    "# Check that `perceptron_output` was used\n",
    "old_perceptron_output = perceptron_output\n",
    "del perceptron_output\n",
    "try:\n",
    "    prediction(inputs, outputs_OR)\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    raise AssertionError(\"prediction does not call the perceptron_output function\")\n",
    "finally:\n",
    "    perceptron_output = old_perceptron_output\n",
    "    del old_perceptron_output\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Once you've finished with your function, you should be able to train a perceptron on the OR outputs and see what it predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "65073d1b7733ea095dd180f48a22f26f",
     "grade": false,
     "grade_id": "compare_or_outputs",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "inputs = create_inputs()\n",
    "outputs_OR = create_outputs_OR()\n",
    "\n",
    "print(\"Correct OR outputs: \" + str(outputs_OR))\n",
    "print(\"Trained OR outputs: \" + str(prediction(inputs, outputs_OR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Was the single-layer perceptron able to learn the OR function? Why is it that the single-layer perceptron can/cannot solve this problem?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0bc700169539646d767d223098c2c34e",
     "grade": true,
     "grade_id": "learn-or",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false
   },
   "source": [
    "---\n",
    "## Part C (0.5 points)\n",
    "\n",
    "Can we learn XOR using the same network architecture? Let's train a new one-layer feedforward neural net using the variables `inputs`, `outpus_XOR`, and `bias = 0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "train_perceptron(inputs, outputs_XOR, bias=0, num_loops=20, plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Using the outputs you computed for XOR, let's run your prediction function to see what the final numerical predictions are for XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "25bfb83c2e06643282079a45c3218222",
     "grade": false,
     "grade_id": "compare_xor_outputs",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "inputs = create_inputs()\n",
    "outputs_XOR = create_outputs_XOR()\n",
    "\n",
    "print(\"Correct XOR outputs: \" + str(outputs_XOR))\n",
    "print(\"Trained XOR outputs: \" + str(prediction(inputs, outputs_XOR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Was the single-layer perceptron able to learn the XOR function? Why does it perform this way?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f19a9cfbfbed9cba35e4a8c2d9e4e721",
     "grade": true,
     "grade_id": "single-layer-XOR",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "## Part D (1 point)\n",
    "\n",
    "Can we learn XOR using a two-layer feedforward network like the following?\n",
    "\n",
    "![Network structure with one hidden layer.](images/NetworkDiagram.png)\n",
    "\n",
    "To find out, we will train a two-layer feedforward network with the function `train_multilayer_perceptron()`. \n",
    "\n",
    "`train_multilayer_perceptron()` takes four arguments:\n",
    "\n",
    "* `X` — the training input matrix\n",
    "* `Y` — the training output vector\n",
    "* `num_iter` — the number of iterations to run \n",
    "\n",
    "`train_multilayer_perceptron()` returns two outputs. The first is a tuple of the weight matrices for the first and second layers, and the second is a tuple of the bias terms for the first and second layers. \n",
    "\n",
    "Using the variables `inputs` and `outputs_XOR` that you used in Part C, we will train a two-layer feedforward neural net (a network with one hidden layer) below. Note that this may take a few moments to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a67409825cc3d9ba824d0aa2e43c2b05",
     "grade": false,
     "grade_id": "train_multilayer_perceptron",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from multilayer_perceptron import train_multilayer_perceptron\n",
    "\n",
    "weights, bias = train_multilayer_perceptron(inputs, outputs_XOR, num_iters=20000)\n",
    "\n",
    "print(\"Layer 1 weights: \" + str(weights[0]))\n",
    "print(\"Layer 1 bias:    \" + str(bias[0]))\n",
    "print(\"Layer 2 weights: \" + str(weights[1]))\n",
    "print(\"Layer 2 bias:    \" + str(bias[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Now let's calculate the predicted output values given the input and the trained model using the function `predict_multilayer_perceptron()`.  `predict_multilayer_perceptron()` is provided because the calculation is somewhat more complex than the analogous operation with single-layer feedforward networks that we asked you to write in Parts B and C. `predict_multilayer_perceptron()` takes three arguments:\n",
    "\n",
    "* `input` — the training input matrix\n",
    "* `weights` — a tuple of weights for each layer\n",
    "* `bias` — a tuple of bias terms for each layer\n",
    "\n",
    "`predict_multilayer_perceptron()` returns a vector of output values given the trained net and the input.\n",
    "\n",
    "Let's compute the prediction for the XOR outputs that we just trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "096de1f18faad5c8b636220542f74047",
     "grade": false,
     "grade_id": "predict_multilayer_perceptron",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from multilayer_perceptron import predict_multilayer_perceptron\n",
    "\n",
    "out = predict_multilayer_perceptron(inputs, weights, bias)\n",
    "\n",
    "print(\"Correct XOR outputs: \" + str(outputs_XOR))\n",
    "print(\"Trained XOR outputs: \" + str(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Was the multi-layer perceptron able to learn the XOR function? Explain why or why not.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "224b861c4122f70c79b53160112e1c1c",
     "grade": true,
     "grade_id": "multi-XOR",
     "locked": false,
     "points": 0.75,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">How does the behavior of the multi-layer perceptron compare to that of the single-layer perceptron on the same input data?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bfc3a238a23807a88f91c610dd0572ce",
     "grade": true,
     "grade_id": "multi-XOR2",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
