{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "outputs": [],
   "source": [
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# special imports for running and evaluating the Elman network\n",
    "from elman import train_Elman, predict_Elman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**N.B.** The ideas in this notebook draw heavily from the readings\n",
    "<ol>\n",
    "<li> Elman, J. L. (1990). Finding structure in time. _Cognitive Science, 14_, 179-211.\n",
    "<li>McClelland, J., & Rumelhart, M. (1986). Past tenses of English verbs. In McClelland, J. and Rumelhart, D. (Eds.) _Parallel distributed processing: Explorations in the microstructure of cognition. Vol. 2: Applications_ (pp. 216-271). Cambridge, MA: MIT Press.\n",
    "</ol>\n",
    "<br>\n",
    "If you are confused about some of the ideas in this notebook or would like further clarification, we recommend having a look there.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "One of the most successful (and controversial) applications of neural\n",
    "networks has been as models of human language. You will test whether a\n",
    "simple neural network is capable of learning the rule underlying a\n",
    "context-free language.\n",
    "\n",
    "The language $a^nb^n$, being the set of all strings containing a\n",
    "sequence of $a$'s followed by a sequence of $b$'s of the same length\n",
    "is a simple example of a language that can be generated by a\n",
    "context-free grammar but not a finite-state grammar. Human languages\n",
    "exhibit similar long-range constraints -- for example, a plural noun\n",
    "at the start of a sentence affects conjugation of a verb at the end,\n",
    "regardless of what intervenes. Some criticisms of applications of\n",
    "neural networks to human languages are based upon their apparent\n",
    "reliance on local sequential structure, which makes them seem much\n",
    "more similar to finite-state grammars than to context-free\n",
    "grammars. An interesting question to explore is thus whether a\n",
    "recurrent neural network can learn to generalize a simple rule\n",
    "characterizing a long-range dependency, such as the rule underlying\n",
    "$a^nb^n$.\n",
    "\n",
    "Recall that an \"Elman\" network, as discussed by Elman (1990), is a\n",
    "recurrent network where the activation of the hidden units at the\n",
    "previous timestep are used as input to the hidden units on the current\n",
    "timestep. This type of network architecture allows the\n",
    "network to learn about sequential dependencies in the input data. In this notebook we will evaluate whether such a network can learn an $a^nb^n$\n",
    "grammar. Here we formalize learning a grammar as being able to correctly\n",
    "predict what the next item in a sequence should be given the\n",
    "rules of the grammar. Therefore, the output node represents the\n",
    "networks's prediction for what the next item in the sequence (the next\n",
    "input) will be -- it outputs a $1$ if it thinks the current input will\n",
    "be followed by an $a$, and outputs a $0$ if it thinks the current\n",
    "input will be followed by a $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Data\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "We will use the `abdata.npz` dataset for this problem. This dataset has two keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "ab_data = np.load(\"data/abdata.npz\")\n",
    "ab_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The array `train_data` contains the sequence we will use to train our network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "train_data = ab_data['train_data']\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The array `test_data` contains the sequence we will use to evaluate the\n",
    "network: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "test_data = ab_data['test_data']\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "In both `train_data` and `test_data` a $1$ represents an $a$ and a $0$ represents a $b$.\n",
    "\n",
    "`train_data` was constructed by concatenating a randomly ordered\n",
    "set of strings of the form $a^nb^n$, with $n$ ranging from 1 to 11.\n",
    "The frequency of sequences for a given value of $n$ in the training set\n",
    "are given by `np.ceil(50/n)`, thus making them inversely proportional to $n$.\n",
    "The `np.ceil` function returns the smallest integer greater or equal to\n",
    "its input. For example, `np.ceil(3)` is 3, but `np.ceil(3.1)` is\n",
    "4 . `test_data` contains an ordered sequence of strings of the form\n",
    "$a^nb^n$, with $n$ increasing from 1 to 18 over the length of the\n",
    "string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Part A (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "In order to train your network, you will need both training input and\n",
    "training output. That is, you need a sequence of inputs of the form\n",
    "$a^nb^n$, and a corresponding sequence with the correct output for\n",
    "each item in the input sequence.\n",
    "\n",
    "For this problem we're going to use `train_data[:-1]` as the\n",
    "input training sequence, and `train_data[1:]` as the output\n",
    "training sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Explain why these are appropriate input and output\n",
    "sequences. If you're confused by what the sequences\n",
    "`train_data[:-1]` and `train_data[1:]` look like,\n",
    "try creating them in a scratch cell and compare them to `train_data`.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "14b047e279089bd4f62ad09256bbcbec",
     "grade": true,
     "grade_id": "part_a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Part B (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "We have provided you with a function, `train_Elman`, which takes four arguments:\n",
    "- `input` -- the training input sequence\n",
    "- `output` -- the training output sequence\n",
    "- `num_hidden` -- the number of hidden units\n",
    "- `num_iters` -- the number of training iterations\n",
    "\n",
    "\n",
    "`train_Elman` will create a network with one input node, the specified number of\n",
    "hidden units, and one output node, and then train it on the training\n",
    "data for the specified number of iterations. The network sees the\n",
    "training data one input at a time (in our case, it sees a single $1$\n",
    "or $0$ per time step).\n",
    "\n",
    "Complete the function `anbn_learner` below to train an \"Elman\"\n",
    "network with two hidden units using the provided function `train_Elman` (remember\n",
    "to use the input and output sequences from Part A). Train the network\n",
    "for 100 iterations, and return the final output of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c780e1b1346209cf913d01a4babea476",
     "grade": false,
     "grade_id": "anbn_learner",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def anbn_learner(train_data):\n",
    "    \"\"\"\n",
    "    Creates an \"Elman\" neural network with two hidden units and trains it\n",
    "    on the provided data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: numpy array of shape (n,)\n",
    "        the data on which to train the Elman network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    net: dictionary with 2 keys\n",
    "        a dictionary containing the weights of the network. Valid keys are 1 and 2. \n",
    "        key 1 is for the weights between the input and the hidden units, and \n",
    "        key 2 is for the weights between the hidden units and the output units.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8474acbd70334d48c94b2a3f9c2ddd76",
     "grade": true,
     "grade_id": "check_anbn_learner",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that anbn_learner returns the correct output\"\"\"\n",
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal, assert_almost_equal \n",
    "\n",
    "# check that abdata hasn't been modified\n",
    "ab = np.load(\"data/abdata.npz\")\n",
    "assert_array_equal(test_data, ab['test_data'], \"test_data array has changed\")\n",
    "assert_array_equal(train_data, ab['train_data'], \"train_data array has changed\")\n",
    "\n",
    "# generate test data\n",
    "traindata = np.zeros(20)\n",
    "traindata[10:] = 1.\n",
    "\n",
    "net = anbn_learner(traindata)\n",
    "\n",
    "# check that net has the correct shape and type\n",
    "assert_equal(type(net), dict, \"net should be a dict of network weights\")\n",
    "assert_equal(len(net), 2, \"incorrect number of layers in net\")\n",
    "assert_equal(list(net.keys()), [1,2], \"keys for net should be 1 and 2\")\n",
    "\n",
    "# check the dimensions of the weight matrices\n",
    "assert_equal(net[1].shape, (2,4), \"invalid network weights for the input -> hidden layer\")\n",
    "assert_equal(net[2].shape, (1,3), \"invalid network weights for the hidden -> output layer\")\n",
    "\n",
    "# check the weight matrix sums to the correct value on testdata\n",
    "assert_almost_equal(np.sum(net[1]), -1.9326, places=4, msg=\"weights for input --> hidden layer are incorrect\")\n",
    "assert_almost_equal(np.sum(net[2]), 0.01825, places=4, msg=\"weights for hidden --> output layer are incorrect\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Once the network is trained, you can test it on a new set of sequences\n",
    "and evaluate its predictions to see how well it has learned the target\n",
    " grammar. To generate predictions from the trained network, we use the provided function `predict_Elman`. Try using your trained network to predict the sequences in `test_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "net = anbn_learner(train_data) # train the Elman network on train_data\n",
    "predictions = predict_Elman(net, test_data) # use the trained network to predict test_data\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Part C (1.25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The squared error (SE) for a prediction $p_i$ in the prediction vector ${\\bf p}$\n",
    "compared to a target value ${y_i}$ in the target vector ${\\bf y}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "SE_i = (p_i-y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "That is, the squared error is just the squared difference between the\n",
    "predicted and target value.\n",
    "\t\n",
    "Complete the function `squared_error`, which takes in an array of test data and an array of\n",
    "predictions. The function should return an error array containing the SE for each\n",
    "of the predictions of the network compared against the corresponding value in `test_data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">Remember that the predictions refer to the _next_ item in the sequence\n",
    "(e.g.  `predictions[0]` should be compared to\n",
    "`test_data[1]`, etc.). You should append an $a$ to the end of your test data to equate the array sizes (describing the start of a new sequence of $a^nb^n$). </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc34fc992d440beb6bea797faabbc3e5",
     "grade": false,
     "grade_id": "sq_err",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def squared_error(predictions, test_data):\n",
    "    \"\"\"\n",
    "    Uses equation 1 to compute the SE for each of the predictions made \n",
    "    by the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: numpy array of shape (n,)\n",
    "        an array of predictions from the Elman network\n",
    "    \n",
    "    test_data: numpy array of shape (n,) \n",
    "        the array of test data from which predictions were generated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    se_vector: numpy array of shape (n,)\n",
    "        an array containing the SE for each of items in predictions \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b208daede991b158bc6b80965cd889e0",
     "grade": true,
     "grade_id": "check_sq_err",
     "locked": false,
     "points": 1.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Check that squared_error returns the correct output\"\"\"\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# generate test data\n",
    "pred = np.array([1, 0, 1])\n",
    "test = np.array([0, 1, 0])\n",
    "se = squared_error(pred, test)\n",
    "\n",
    "# check that squared_error returns the correct output for testdata\n",
    "assert_equal(se.dtype, np.float64, \"squared_error should return an array of floats\")\n",
    "assert_equal(se.shape, (3,), \"squared_error returned an array of the incorrect size on the validate testdata\")\n",
    "assert_array_equal(se, np.zeros(3), \"squared_error should return all zeros on the validate testdata\")\n",
    "\n",
    "# check that squared_error compares the correct elements\n",
    "pred = np.zeros(1)\n",
    "test = np.zeros(1)\n",
    "se = squared_error(pred, test)\n",
    "assert_equal(se, np.ones(1), \"squared_error([0],[0]) should have returned a 1 (did you remember to append an a to testdata?\")\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Part D (1.25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Use matplotlib to plot a bar graph of the squared error for each training example. Don't forget to provide a title and label your $x$ and $y$ axes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">If you have difficulty interpreting this graph, you may want to\n",
    "examine a few of the values in `test_data`, `predictions`, and your `mse_vector` to see how they are related.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "361a23caac2e11f8082d5b4f29f2e4be",
     "grade": false,
     "grade_id": "plot_error",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize network and train on train_data\n",
    "net = anbn_learner(train_data)\n",
    "predictions = predict_Elman(net, test_data)\n",
    "\n",
    "# use the trained network to predict test_data\n",
    "se_vector = squared_error(predictions, test_data)\n",
    "\n",
    "# create the figure\n",
    "fig, axis = plt.subplots()\n",
    "axis.set_xlim([0.0, 350.0]), axis.set_ylim([0.0,.7])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "To get a better idea of what is going on, let's have a look at the values in `test_data` where the prediction error spikes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# prints the 3 values preceding and 2 values following the spot where \n",
    "# the prediction error >= 0.5\n",
    "error_spike_idxs = np.argwhere(se_vector >= 0.5) + 1\n",
    "error_spike_idxs = error_spike_idxs[:-1]\n",
    "\n",
    "for i in error_spike_idxs:\n",
    "    print('3 values preceding MSE spike: {}\\tValue at MSE spike: {}'\n",
    "          '\\t\\t2 values following MSE spike: {}'\\\n",
    "          .format(test_data[i[0]-3:i[0]], test_data[i[0]], test_data[i[0]+1:i[0]+3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "977af31c8d9065f633ea60864b920288",
     "grade": true,
     "grade_id": "check_plot_error",
     "locked": false,
     "points": 1.25,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal, assert_allclose \n",
    "from nose.tools import assert_equal, assert_not_equal    \n",
    "\n",
    "# check that a title and labels were included\n",
    "assert_not_equal(axis.get_title(), '', \"no title given\")\n",
    "assert_not_equal(axis.get_xlabel(), '', \"no x label set\")\n",
    "assert_not_equal(axis.get_ylabel(), '', \"no y label set\")\n",
    "\n",
    "# check that axis limits are correct\n",
    "assert_equal(axis.get_xlim(), (0.0, 350.0), \"incorrect x-axis limits\")\n",
    "[assert_allclose(*x, err_msg=\"incorrect y-axis limits\") for x in zip(axis.get_ylim(), np.array([0.0, 0.7]))]\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Part E (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Earlier we said that we can evaluate whether the network has learned\n",
    "the grammar by looking at the predictions it makes. If the network has\n",
    "learned the $a^nb^n$ grammar, in what cases should it make correct\n",
    "predictions? When should it make incorrect predictions? (**1 point**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d5d9c21b21fab72de1af0b186f6147dc",
     "grade": true,
     "grade_id": "part_e1",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">Do your predictions about when the network should make correct/incorrect predictions if it has learned the $a^nb^n$ grammar match the MSE values you plotted in Part D? (**1 point**)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9563ec8072df0a641f17419714fd8013",
     "grade": true,
     "grade_id": "part_e2",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "---\n",
    "\n",
    "## Part F (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "<div class=\"alert alert-success\">At what level of the Chomsky hierarchy is the $a^nb^n$ grammar? How does this compare to the level of most natural languages? Use this to explore the implications of your results from Part E for using the Elman network to model the relationships present in human language.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9a115df07d13ef2cc57599d57ad0c123",
     "grade": true,
     "grade_id": "part_f",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "source": [
    "Before turning this problem in remember to do the following steps:\n",
    "\n",
    "1. **Restart the kernel** (Kernel$\\rightarrow$Restart)\n",
    "2. **Run all cells** (Cell$\\rightarrow$Run All)\n",
    "3. **Save** (File$\\rightarrow$Save and Checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "release"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">After you have completed these three steps, ensure that the following cell has printed \"No errors\". If it has <b>not</b> printed \"No errors\", then your code has a bug in it and has thrown an error! Make sure you fix this error before turning in your problem set.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {}
   },
   "outputs": [],
   "source": [
    "print(\"No errors!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
